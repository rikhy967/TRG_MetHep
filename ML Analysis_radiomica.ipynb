{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd7ec73-9a45-4d8f-bd24-0652152656f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riccardo/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#model\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,PowerTransformer\n",
    "from sklearn.feature_selection import f_classif,mutual_info_classif,VarianceThreshold,SequentialFeatureSelector, SelectPercentile\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score,recall_score, roc_auc_score\n",
    "\n",
    "import optuna\n",
    "from imblearn.over_sampling import SMOTE,ADASYN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e5afac2-e3b2-4791-a41f-cba773170dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with clinical Data\n",
    "\n",
    "clinical_db_center1 = pd.read_excel('clinical_center1.xlsx',index_col=0)\n",
    "clinical_db_center2 = pd.read_excel('clinical_center2.xlsx',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af5deec6-e11d-47fe-a395-cbc94767928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center 1\n",
    "\n",
    "HBP_core_center1 = pd.read_excel('center1_HBP_core.xlsx',index_col=0)\n",
    "selected_cols = HBP_core_center1.columns\n",
    "\n",
    "\n",
    "HBP_ring_center1 = pd.read_excel('center1_HBP_ring.xlsx',index_col=0)[selected_cols]\n",
    "PVP_core_center1 = pd.read_excel('center1_PVP_core.xlsx',index_col=0)[selected_cols]\n",
    "PVP_ring_center1 = pd.read_excel('center1_PVP_ring.xlsx',index_col=0)[selected_cols]\n",
    "\n",
    "\n",
    "HBP_core_center1['TRG'] = [1 if x<=3 else 0 for x in clinical_db_center1['TRG']]\n",
    "HBP_ring_center1['TRG'] = [1 if x<=3 else 0 for x in clinical_db_center1['TRG']]\n",
    "PVP_core_center1['TRG'] = [1 if x<=3 else 0 for x in clinical_db_center1['TRG']]\n",
    "PVP_ring_center1['TRG'] = [1 if x<=3 else 0 for x in clinical_db_center1['TRG']]\n",
    "\n",
    "\n",
    "\n",
    "# Center 2\n",
    "\n",
    "HBP_core_center2 = pd.read_excel('center2_HBP_core.xlsx',index_col=0)\n",
    "selected_cols = HBP_core_center2.columns\n",
    "\n",
    "\n",
    "HBP_ring_center2 = pd.read_excel('center2_HBP_ring.xlsx',index_col=0)[selected_cols]\n",
    "PVP_core_center2 = pd.read_excel('center2_PVP_core.xlsx',index_col=0)[selected_cols]\n",
    "PVP_ring_center2 = pd.read_excel('center2_PVP_ring.xlsx',index_col=0)[selected_cols]\n",
    "\n",
    "\n",
    "HBP_core_center2['TRG'] = [1 if x<=3 else 0 for x in clinical_db_center2['TRG']]\n",
    "HBP_ring_center2['TRG'] = [1 if x<=3 else 0 for x in clinical_db_center2['TRG']]\n",
    "PVP_core_center2['TRG'] = [1 if x<=3 else 0 for x in clinical_db_center2['TRG']]\n",
    "PVP_ring_center2['TRG'] = [1 if x<=3 else 0 for x in clinical_db_center2['TRG']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28c12db3-cbe0-414c-b8f7-55e7d8cd74cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICH\n",
    "\n",
    "\n",
    "\n",
    "HBP_core = pd.concat([HBP_core_center1,HBP_core_center2])\n",
    "HBP_ring = pd.concat([HBP_ring_center1,HBP_ring_center2])\n",
    "PVP_core = pd.concat([PVP_core_center1,PVP_core_center2])\n",
    "PVP_ring = pd.concat([PVP_ring_center1,PVP_ring_center2])\n",
    "\n",
    "HBP = HBP_core.join(HBP_ring.drop(['TRG'],axis = 1), rsuffix='_ring')\n",
    "PVP = PVP_core.join(PVP_ring.drop(['TRG'],axis = 1), rsuffix='_ring')\n",
    "\n",
    "ring = HBP_ring.join(PVP_ring.drop(['TRG'],axis = 1), rsuffix='_pvp')\n",
    "core = HBP_core.join(PVP_core.drop(['TRG'],axis = 1), rsuffix='_pvp')\n",
    "\n",
    "total = HBP_core.join(HBP_ring.drop(['TRG'],axis = 1), rsuffix='_hbp_ring')\\\n",
    "                   .join(PVP_core.drop(['TRG'],axis = 1), rsuffix='_pvp_core')\\\n",
    "                   .join(PVP_ring.drop(['TRG'],axis = 1), rsuffix='_pvp_pring')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff274fcd-2fbb-440d-8a94-9e04e193d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "\n",
    "class MyDecorrelator(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "        self.correlated_columns = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        correlated_features = set()  \n",
    "        X = pd.DataFrame(X)\n",
    "        corr_matrix = X.corr()\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i):\n",
    "                if abs(corr_matrix.iloc[i, j]) > self.threshold: # we are interested in absolute coeff value\n",
    "                    colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                    correlated_features.add(colname)\n",
    "        self.correlated_features = correlated_features\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **kwargs):\n",
    "        return (pd.DataFrame(X)).drop(labels=self.correlated_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ebcbe8f-3df9-4164-bca2-7f2b165e07d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = epato_core.copy()\n",
    "\n",
    "# train-test-validation esterna\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "def dataset_split(dataset):\n",
    "\n",
    "    dt = dataset.drop(['TRG'],axis = 1)\n",
    "    X_interno, X_esterno, y_interno, y_esterno = train_test_split(dt, dataset['TRG'], test_size=0.33, random_state=42, stratify= dataset['TRG'])\n",
    "    return X_interno,y_interno,X_esterno,y_esterno\n",
    "\n",
    "# Optimization module\n",
    "\n",
    "def objective(trial,X,y):\n",
    "\n",
    "    \n",
    "    # Standardization\n",
    "    \n",
    "\n",
    "    \n",
    "    std = trial.suggest_categorical('standard', ['StandardScaler', 'MinMaxScaler', 'PowerTransformer'])\n",
    "    \n",
    "    if std=='StandardScaler':\n",
    "        scaler = StandardScaler()\n",
    "    elif std=='MinMaxScaler':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        scaler = PowerTransformer()\n",
    "    \n",
    "\n",
    "    dec_perc = trial.suggest_float('dec_perc',0.8,0.99, step = 0.02)\n",
    "    dec = MyDecorrelator(dec_perc)\n",
    "\n",
    "    \n",
    "    # Feature selection\n",
    "    \n",
    "    fs = trial.suggest_categorical('feature_selection', [  'SelectPercentile'])\n",
    "    \n",
    "    if fs=='VarianceThreshold':\n",
    "        threshold_fs = trial.suggest_float('threshold_fs',0,1, step = 0.05)\n",
    "        selector = VarianceThreshold(threshold_fs)\n",
    "\n",
    "    elif fs=='SequentialFeatureSelector':\n",
    "        estim = LogisticRegression(n_jobs=-1)\n",
    "        n_features_to_select_fs = trial.suggest_int('n_features_to_select_fs',4,20)\n",
    "        direction_fs = trial.suggest_categorical('direction_fs', ['forward'])\n",
    "        selector = SequentialFeatureSelector(estim,\n",
    "                                             n_features_to_select = n_features_to_select_fs,\n",
    "                                             direction = direction_fs,scoring = 'roc_auc',n_jobs = -1)\n",
    "\n",
    "    else:\n",
    "        percentile_fs = trial.suggest_float('percentile_fs',5,15, step = 1)\n",
    "        selector = SelectPercentile(mutual_info_classif,percentile = percentile_fs)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Model\n",
    "    m = trial.suggest_categorical('model', ['SGDClassifier','LinearSVC','AdaBoostClassifier','RandomForestClassifier','GradientBoostingClassifier'])\n",
    "    \n",
    "    if m == 'SGDClassifier':\n",
    "        loss_sgd = trial.suggest_float('loss_sgd',1e-2,1e2, step = 0.001)\n",
    "        m = LogisticRegression(C=loss_sgd,solver = 'liblinear',class_weight='balanced')\n",
    "    elif m == 'LinearSVC':\n",
    "        m = LinearSVC(class_weight='balanced')\n",
    "    elif m == 'AdaBoostClassifier':\n",
    "        criterion_ada = trial.suggest_categorical('criterion_ada', ['gini', 'entropy'])\n",
    "        max_depth_ada = trial.suggest_int('max_depth_ada',2,5)\n",
    "        base_estimator = DecisionTreeClassifier(criterion = criterion_ada,\n",
    "                                                max_depth=max_depth_ada,class_weight='balanced')\n",
    "        n_estimators_ada = trial.suggest_int('n_estimators_ada',3,10)\n",
    "        learning_rate_ada = trial.suggest_float('learning_rate_ada',0.1,10)\n",
    "        m = AdaBoostClassifier(estimator = base_estimator, n_estimators=n_estimators_ada,\n",
    "                               learning_rate=learning_rate_ada,random_state=4)\n",
    "    elif m == 'RandomForestClassifier':\n",
    "        n_estimators_rf =trial.suggest_int('n_estimators_rf',300,500)\n",
    "        criterion=trial.suggest_categorical('criterion_rf', ['gini', 'entropy'])\n",
    "        max_depth=trial.suggest_int('max_depth_rf',2,5)\n",
    "        min_samples_split=trial.suggest_int('min_samples_split_rf',2,10)\n",
    "        min_samples_leaf=trial.suggest_int('min_samples_leaf_rf',2,10)\n",
    "        max_features=trial.suggest_float('max_features_rf',0.3,0.6)\n",
    "        #ccp_alpha=trial.suggest_float('ccp_alpha_rf',1e-5,1)\n",
    "        max_samples=trial.suggest_float('max_samples_rf',0.5,0.9)\n",
    "        #print(n_estimators_rf)\n",
    "        m = RandomForestClassifier(n_estimators=n_estimators_rf,\n",
    "                                   criterion=criterion,\n",
    "                                   max_depth=max_depth,\n",
    "                                   #min_samples_leaf=min_samples_leaf,\n",
    "                                   #min_samples_split = min_samples_split,\n",
    "                                   max_features=max_features,\n",
    "                                   #max_samples=max_samples,\n",
    "                                   #class_weight='balanced_subsample',\n",
    "                                   n_jobs=-1)\n",
    "    else:\n",
    "        m = GradientBoostingClassifier()\n",
    "        \n",
    "    cv = StratifiedKFold(5)\n",
    "    cvc = CalibratedClassifierCV(m, cv=cv, method=\"sigmoid\")\n",
    "    pipe = Pipeline([('scaler',scaler),('decor',dec),('selector',selector),('model',m)])\n",
    "    \n",
    "    accuracy = cross_val_score(pipe,X,y, scoring='roc_auc',cv = RepeatedKFold(n_splits=3, n_repeats=2,), n_jobs=-1,error_score='raise')\n",
    "    \n",
    "    \n",
    "    return np.mean(accuracy)\n",
    "\n",
    "# 3. Create a study object and optimize the objective function.\n",
    "def train(X,y):\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    func = lambda trial: objective(trial, X, y)\n",
    "    study = optuna.create_study(direction='maximize',sampler=optuna.samplers.TPESampler())\n",
    "    study.optimize(func, n_trials=100)\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48d02840-30b2-432a-a527-fd9aead872f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(best_params,X_interno,y_interno,X_esterno,y_esterno):\n",
    "\n",
    "    std = best_params['standard']\n",
    "    if std=='StandardScaler':\n",
    "            scaler = StandardScaler()\n",
    "    elif std=='MinMaxScaler':\n",
    "            scaler = MinMaxScaler()\n",
    "    else:\n",
    "            scaler = PowerTransformer()\n",
    "\n",
    "\n",
    "    dec = MyDecorrelator(best_params['dec_perc'])\n",
    "        # Feature selection\n",
    "\n",
    "    fs = best_params['feature_selection']\n",
    "\n",
    "    if fs=='VarianceThreshold':\n",
    "            threshold_fs = best_params['threshold_fs']\n",
    "            selector = VarianceThreshold(threshold_fs)\n",
    "    elif fs=='SequentialFeatureSelector':\n",
    "            estim = LogisticRegression(n_jobs=-1)\n",
    "            n_features_to_select_fs = best_params['n_features_to_select_fs']\n",
    "            direction_fs = best_params['direction_fs']\n",
    "            selector = SequentialFeatureSelector(estim,\n",
    "                                                 n_features_to_select = n_features_to_select_fs,\n",
    "                                                 direction = direction_fs,scoring = 'roc_auc',n_jobs = -1)\n",
    "\n",
    "    else:\n",
    "            percentile_fs = best_params['percentile_fs']\n",
    "            selector = SelectPercentile(mutual_info_classif,percentile = percentile_fs)\n",
    "\n",
    "\n",
    "        # Model\n",
    "    m = best_params['model']\n",
    "\n",
    "    if m == 'SGDClassifier':\n",
    "            loss_sgd = best_params['loss_sgd']\n",
    "            m = LogisticRegression(C=loss_sgd,solver = 'liblinear',class_weight='balanced',n_jobs=-1)\n",
    "    elif m == 'LinearSVC':\n",
    "            m = LinearSVC(class_weight='balanced')\n",
    "    elif m == 'AdaBoostClassifier':\n",
    "            base_estimator = DecisionTreeClassifier(criterion = best_params['criterion_ada'],\n",
    "                                                    max_depth=best_params['max_depth_ada'])\n",
    "\n",
    "            m = AdaBoostClassifier(estimator = base_estimator, n_estimators=best_params['n_estimators_ada'],\n",
    "                                   learning_rate=best_params['learning_rate_ada'],random_state=4)\n",
    "            #m = AdaBoostClassifier()\n",
    "    elif m == 'RandomForestClassifier':\n",
    "            \n",
    "            m = RandomForestClassifier(n_estimators=best_params['n_estimators_rf'],\n",
    "                                   criterion=best_params['criterion_rf'],\n",
    "                                    max_depth=best_params['max_depth_rf'],\n",
    "                                   #min_samples_leaf=best_params['min_samples_leaf_rf'],\n",
    "                                   #min_samples_split = best_params['min_samples_split_rf'],\n",
    "                                   max_features=best_params['max_features_rf'],\n",
    "                                   #max_samples=best_params['max_samples_rf'],\n",
    "                                   #ccp_alpha=best_params['ccp_alpha_rf'],\n",
    "                                   #class_weight='balanced_subsample',\n",
    "                                       n_jobs=-1)\n",
    "    else:\n",
    "            m = GradientBoostingClassifier()\n",
    "\n",
    "    cv = StratifiedKFold(5)\n",
    "    cvc = CalibratedClassifierCV(m, cv=cv, method=\"sigmoid\")\n",
    "    \n",
    "    pipe = Pipeline([('scaler',scaler),('decor',dec),('selector',selector),('model',m)])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    cvc = pipe.fit(X_interno,y_interno)\n",
    "    print('Train\\n')\n",
    "\n",
    "    y_pred_train = cvc.predict(X_interno)\n",
    "    y_pred_proba_train = cvc.predict_proba(X_interno)\n",
    "\n",
    "    \n",
    "    acc = accuracy_score(y_interno,y_pred_train)\n",
    "    sens = recall_score(y_interno,y_pred_train)\n",
    "    spec = recall_score(y_interno,y_pred_train,pos_label=0)\n",
    "    auc = roc_auc_score(y_interno,y_pred_proba_train[:,1])\n",
    "\n",
    "    \n",
    "    #scores = cross_val_score(pipe,X, y, cv=10,scoring='accuracy')\n",
    "    #print(\"Accuracy: %0.2f (%0.2f-%0.2f)\" % (scores.mean(), np.quantile(scores,q= 0.25),np.quantile(scores,q= 0.75)))\n",
    "\n",
    "    \n",
    "    print('Acc = ',round(acc,3),' Sens = ',round(sens,3), ' Spec = ',round(spec,3),' AUC = ',round(auc,3))\n",
    "    print('\\n\\n')\n",
    "    print('Test\\n')\n",
    "\n",
    "    y_pred_test = cvc.predict(X_esterno)\n",
    "    y_pred_proba_test = cvc.predict_proba(X_esterno)\n",
    "    acc = accuracy_score(y_esterno,y_pred_test)\n",
    "    sens = recall_score(y_esterno,y_pred_test)\n",
    "    spec = recall_score(y_esterno,y_pred_test,pos_label=0)\n",
    "    auc = roc_auc_score(y_esterno,y_pred_proba_test[:,1])\n",
    "    print('Acc = ',round(acc,3),' Sens = ',round(sens,3), ' Spec = ',round(spec,3),' AUC = ',round(auc,3))\n",
    "    return cvc,m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2384aeb9-d679-4d94-b582-94973c3119b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Dataset Analysis:  Totale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riccardo/miniconda3/lib/python3.9/site-packages/optuna/distributions.py:685: UserWarning: The distribution is specified by [0.8, 0.99] and step=0.02, but the range is not divisible by `step`. It will be replaced by [0.8, 0.98].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'standard': 'MinMaxScaler',\n",
       " 'dec_perc': 0.88,\n",
       " 'feature_selection': 'SelectPercentile',\n",
       " 'percentile_fs': 10.0,\n",
       " 'model': 'SGDClassifier',\n",
       " 'loss_sgd': 2.1039999999999996}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "\n",
      "Acc =  0.752  Sens =  0.717  Spec =  0.782  AUC =  0.806\n",
      "\n",
      "\n",
      "\n",
      "Test\n",
      "\n",
      "Acc =  0.686  Sens =  0.652  Spec =  0.714  AUC =  0.691\n",
      "\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riccardo/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 24.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "list_datasets = [epato_core,epato_ring,portale_core,portale_ring, core, ring, epato, portale, totale]\n",
    "dict_results = {}\n",
    "\n",
    "for i, dataset in enumerate(list_datasets):\n",
    "    if i == 0:\n",
    "        t = 'HBP_core'\n",
    "        \n",
    "    elif i == 1:\n",
    "        t = 'HBP_ring'\n",
    "        \n",
    "    elif i == 2:\n",
    "        t = 'PVP_core'\n",
    "        \n",
    "    elif i == 3:\n",
    "        t = 'PVP_ring'\n",
    "        \n",
    "    elif i == 4:\n",
    "        t = 'Core total'\n",
    "        \n",
    "    elif i == 5:\n",
    "        t = 'Ring total'\n",
    "        \n",
    "    elif i == 6:\n",
    "        t = 'HBP totale'\n",
    "        \n",
    "    elif i == 7:\n",
    "        t = 'Po totale'\n",
    "        \n",
    "    else:\n",
    "        t = 'Totale'\n",
    "        \n",
    "    if t=='Totale':\n",
    "        \n",
    "        print('-- Dataset Analysis: ',t)\n",
    "        X_interno,y_interno,X_esterno,y_esterno = dataset_split(dataset)\n",
    "        study = train(X_interno,y_interno)\n",
    "        display(study.best_params)\n",
    "        cvc, m = evaluate(study.best_params,X_interno,y_interno,X_esterno,y_esterno)\n",
    "        s = pickle.dump(cvc,open(t+'_CalibratedClassifier_Radiomics.pkl', 'wb'))\n",
    "        s = pickle.dump(cvc,open(t+'_Classifier_Radiomics.pkl', 'wb'))\n",
    "        \n",
    "\n",
    "        print('\\n ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c73f6-4944-4de3-a39e-46542485b4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
